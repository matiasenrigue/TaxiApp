{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dc1c241",
   "metadata": {},
   "source": [
    "## Monthly Models\n",
    "\n",
    "Last step when creating and making the model must have files that have both hotness and trip duration in the dataset this notebook will be used for the sliding window method to train all months in 2023 all files when run will be taken locally and saved locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8ffb9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import pickle\n",
    "import json\n",
    "import joblib\n",
    "import os\n",
    "from tkinter import Tk\n",
    "from tkinter.filedialog import askopenfilename\n",
    "from tkinter.filedialog import asksaveasfilename\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30966902",
   "metadata": {},
   "source": [
    "## Monthly Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0564e52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select Month A CSV (used for training and variability calculation):\n",
      "Select Month B CSV (used to calculate hotness):\n",
      "\n",
      "Loaded Month A (Nov): 3072802 rows\n",
      "Loaded Month B (Dec): 3147996 rows\n",
      "\n",
      "Select where to save the hotness table\n",
      "Saved hotness table: C:/diksha/Summer Sem/ScoringModel/Models/December/hotness_table_dec.csv\n",
      "\n",
      "Select where to save the trip duration stats table\n",
      "Saved duration stats: C:/diksha/Summer Sem/ScoringModel/Models/December/duration_variability_nov_dec.csv\n",
      "\n",
      "Select location to save the final dataset\n",
      "Final file saved: C:/diksha/Summer Sem/ScoringModel/Data/Hotness and Duration/Data_with_Features_Nov_Dec.csv\n",
      "Final shape: (6220798, 35)\n"
     ]
    }
   ],
   "source": [
    "def run_feature_engineering_pipeline():\n",
    "   # File selection\n",
    "    Tk().withdraw()\n",
    "    print(\"Select Month A CSV (used for training and variability calculation):\")\n",
    "    file_a = askopenfilename()\n",
    "    print(\"Select Month B CSV (used to calculate hotness):\")\n",
    "    file_b = askopenfilename()\n",
    "\n",
    "    # Read files safely as strings\n",
    "    df_a = pd.read_csv(file_a, dtype=str)\n",
    "    df_b = pd.read_csv(file_b, dtype=str)\n",
    "\n",
    "    nyc_tz = pytz.timezone(\"America/New_York\")\n",
    "\n",
    "    for df in [df_a, df_b]:\n",
    "        df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'], errors='coerce')\n",
    "        df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'], errors='coerce')\n",
    "\n",
    "        # Localize naive timestamps to NYC time (assumes original times are NYC)\n",
    "        df['tpep_pickup_datetime'] = df['tpep_pickup_datetime'].dt.tz_localize(nyc_tz, ambiguous='NaT', nonexistent='shift_forward')\n",
    "        df['tpep_dropoff_datetime'] = df['tpep_dropoff_datetime'].dt.tz_localize(nyc_tz, ambiguous='NaT', nonexistent='shift_forward')\n",
    "\n",
    "\n",
    "    # # Confirm datetime parsing\n",
    "    # print(\"Month A pickup dtype:\", df_a['tpep_pickup_datetime'].dtype)\n",
    "    # print(\"Month B pickup dtype:\", df_b['tpep_pickup_datetime'].dtype)\n",
    "    # print(\"Month B NaT values:\", df_b['tpep_pickup_datetime'].isna().sum())\n",
    "\n",
    "    # Extract month names early\n",
    "    month_a = int(df_a['tpep_pickup_datetime'].dropna().dt.month.mode()[0])\n",
    "    month_b = int(df_b['tpep_pickup_datetime'].dropna().dt.month.mode()[0])\n",
    "    name_a = pd.to_datetime(f\"2023-{month_a}-01\").strftime('%b')\n",
    "    name_b = pd.to_datetime(f\"2023-{month_b}-01\").strftime('%b')\n",
    "\n",
    "    print(f\"\\nLoaded Month A ({name_a}): {df_a.shape[0]} rows\")\n",
    "    print(f\"Loaded Month B ({name_b}): {df_b.shape[0]} rows\")\n",
    "\n",
    "    # Combine both months\n",
    "    df_all = pd.concat([df_a, df_b], ignore_index=True)\n",
    "\n",
    "    # -------------------------------------\n",
    "    # Dropoff Zone Hotness Table\n",
    "    # -------------------------------------\n",
    "    df_all['pickup_day_of_week'] = df_all['tpep_pickup_datetime'].dt.dayofweek\n",
    "    df_all['pickup_hour'] = df_all['tpep_pickup_datetime'].dt.hour\n",
    "\n",
    "    df_hotness_source = df_all.dropna(subset=['pickup_day_of_week', 'pickup_hour', 'dropoff_zone'])\n",
    "\n",
    "    hotness_table = (\n",
    "        df_hotness_source\n",
    "        .groupby(['dropoff_zone', 'pickup_day_of_week', 'pickup_hour'])\n",
    "        .size()\n",
    "        .reset_index(name='dropoff_zone_hotness')\n",
    "    )\n",
    "\n",
    "    # Save hotness table\n",
    "    print(\"\\nSelect where to save the hotness table\")\n",
    "    hotness_path = asksaveasfilename(\n",
    "        initialfile=f\"hotness_table_{name_b.lower()}.csv\",\n",
    "        defaultextension=\".csv\",\n",
    "        filetypes=[(\"CSV files\", \"*.csv\")]\n",
    "    )\n",
    "    if hotness_path:\n",
    "        hotness_table.to_csv(hotness_path, index=False)\n",
    "        print(f\"Saved hotness table: {hotness_path}\")\n",
    "    else:\n",
    "        print(\"Hotness table save cancelled.\")\n",
    "\n",
    "    # Apply hotness to a dataset\n",
    "    def apply_hotness(df):\n",
    "        df['dropoff_day_of_week'] = df['tpep_dropoff_datetime'].dt.dayofweek\n",
    "        df['dropoff_hour'] = df['tpep_dropoff_datetime'].dt.hour\n",
    "        df = df.merge(\n",
    "            hotness_table,\n",
    "            left_on=['dropoff_zone', 'dropoff_day_of_week', 'dropoff_hour'],\n",
    "            right_on=['dropoff_zone', 'pickup_day_of_week', 'pickup_hour'],\n",
    "            how='left'\n",
    "        )\n",
    "        df['dropoff_zone_hotness'] = df['dropoff_zone_hotness'].fillna(0)\n",
    "        df['log_dropoff_zone_hotness'] = np.log1p(df['dropoff_zone_hotness'])\n",
    "        df.drop(columns=['pickup_day_of_week', 'pickup_hour'], errors='ignore', inplace=True)\n",
    "        return df\n",
    "\n",
    "    df_a = apply_hotness(df_a)\n",
    "    df_b = apply_hotness(df_b)\n",
    "\n",
    "    # -------------------------------------\n",
    "    # Trip Duration Variability Table\n",
    "    # -------------------------------------\n",
    "   # Ensure trip_duration_min is numeric\n",
    "    df_all['trip_duration_min'] = pd.to_numeric(df_all['trip_duration_min'], errors='coerce')\n",
    "\n",
    "    # Drop rows with missing duration before grouping\n",
    "    duration_group_cols = ['pickup_zone', 'dropoff_zone', 'pickup_day_of_week', 'pickup_hour']\n",
    "    duration_stats = (\n",
    "        df_all.dropna(subset=['trip_duration_min'])\n",
    "        .groupby(duration_group_cols)['trip_duration_min']\n",
    "        .agg(['mean', 'std'])\n",
    "        .reset_index()\n",
    "        .rename(columns={'std': 'trip_duration_variability'})\n",
    "    )\n",
    "\n",
    "    # Save duration variability stats\n",
    "    print(\"\\nSelect where to save the trip duration stats table\")\n",
    "    duration_path = asksaveasfilename(\n",
    "        initialfile=f\"duration_variability_{name_b.lower()}.csv\",\n",
    "        defaultextension=\".csv\",\n",
    "        filetypes=[(\"CSV files\", \"*.csv\")]\n",
    "    )\n",
    "    if duration_path:\n",
    "        duration_stats.to_csv(duration_path, index=False)\n",
    "        print(f\"Saved duration stats: {duration_path}\")\n",
    "    else:\n",
    "        print(\"Duration stats save cancelled.\")\n",
    "\n",
    "    def apply_variability(df):\n",
    "        df['pickup_day_of_week'] = df['tpep_pickup_datetime'].dt.dayofweek\n",
    "        df['pickup_hour'] = df['tpep_pickup_datetime'].dt.hour\n",
    "        df = df.merge(duration_stats, on=duration_group_cols, how='left')\n",
    "        df['trip_duration_variability'] = df['trip_duration_variability'].fillna(0)\n",
    "        return df\n",
    "\n",
    "    df_a = apply_variability(df_a)\n",
    "    df_b = apply_variability(df_b)\n",
    "\n",
    "    # -------------------------------------\n",
    "    # Final Save\n",
    "    # -------------------------------------\n",
    "    df_a['pickup_date'] = df_a['tpep_pickup_datetime'].dt.date\n",
    "    df_b['pickup_date'] = df_b['tpep_pickup_datetime'].dt.date\n",
    "    df_final = pd.concat([df_a, df_b], ignore_index=True)\n",
    "\n",
    "    print(\"\\nSelect location to save the final dataset\")\n",
    "    final_path = asksaveasfilename(\n",
    "        initialfile=f\"Data_with_Features_{name_a}_{name_b}.csv\",\n",
    "        defaultextension=\".csv\",\n",
    "        filetypes=[(\"CSV files\", \"*.csv\")]\n",
    "    )\n",
    "    if final_path:\n",
    "        df_final.to_csv(final_path, index=False)\n",
    "        print(f\"Final file saved: {final_path}\")\n",
    "        print(f\"Final shape: {df_final.shape}\")\n",
    "    else:\n",
    "        print(\"Final data save cancelled.\")\n",
    "\n",
    "# -------------------------------------\n",
    "# RUN IT\n",
    "# -------------------------------------\n",
    "run_feature_engineering_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44be696e",
   "metadata": {},
   "source": [
    "## Scoring and Training Code (XGBoost + LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f948979f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: C:/diksha/Summer Sem/ScoringModel/Data/Hotness and Duration/Data_with_Features_Jun_Jul.csv\n",
      " Training months: 6 → 7\n",
      "Train size: 3084442 | Test size: 2717185\n",
      "\n",
      " XGB Model Results:\n",
      "R² Score: 0.3389\n",
      "MAE: 0.1859\n",
      " XGB model saved to: C:/diksha/Summer Sem/ScoringModel/Models/July/model_july_xgb.pkl\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071694 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 573\n",
      "[LightGBM] [Info] Number of data points in the train set: 3084442, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 1.210974\n",
      "\n",
      " LGB Model Results:\n",
      "R² Score: 0.3283\n",
      "MAE: 0.1877\n",
      " LGB model saved to: C:/diksha/Summer Sem/ScoringModel/Models/July/model_july_lgb.pkl\n",
      " Weights saved to: C:/diksha/Summer Sem/ScoringModel/Models/July/scoring_weights_july.json\n",
      "\n",
      " Score Normalization Range (5th–95th percentile): min = 1.00, max = 1.56\n",
      " Raw Predicted Score Stats:\n",
      "  Min: 0.2333, Max: 2.4479\n",
      " Saved normalization range: min = 1.00, max = 1.56\n",
      " Scaler saved to: C:/diksha/Summer Sem/ScoringModel/Models/July/scaler_july.json\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# 1. FEATURE PREP FUNCTION\n",
    "# -------------------------------------\n",
    "def prepare_model_data(df):\n",
    "    df = df.copy()\n",
    "    df['pickup_date'] = pd.to_datetime(df['pickup_date'])\n",
    "\n",
    "    # sin/cos hour\n",
    "    df['sin_hour'] = np.sin(2 * np.pi * df['pickup_hour'] / 24)\n",
    "    df['cos_hour'] = np.cos(2 * np.pi * df['pickup_hour'] / 24)\n",
    "    df.drop(columns=['pickup_hour', 'time_of_day'], inplace=True, errors='ignore')\n",
    "\n",
    "    # Ensure fare_per_minute is usable\n",
    "    df = df.dropna(subset=['fare_per_minute'])  # required target\n",
    "\n",
    "    return df\n",
    "\n",
    "# -------------------------------------\n",
    "# 2. TRAIN + EVAL FUNCTION\n",
    "# -------------------------------------\n",
    "def train_model(train_df, test_df, model_type='xgb', month_str=None):\n",
    "    categorical_cols = ['is_airport_trip', 'pickup_borough', 'dropoff_borough']\n",
    "    numeric_cols = [\n",
    "        'dropoff_zone_hotness', 'is_weekend',\n",
    "        'trip_duration_variability', 'sin_hour', 'cos_hour'\n",
    "    ]\n",
    "\n",
    "    # One-hot encode\n",
    "    X_train_cat = pd.get_dummies(train_df[categorical_cols], drop_first=True)\n",
    "    X_test_cat = pd.get_dummies(test_df[categorical_cols], drop_first=True)\n",
    "    X_test_cat = X_test_cat.reindex(columns=X_train_cat.columns, fill_value=0)\n",
    "\n",
    "    X_train = pd.concat([train_df[numeric_cols].reset_index(drop=True), X_train_cat.reset_index(drop=True)], axis=1)\n",
    "    X_test = pd.concat([test_df[numeric_cols].reset_index(drop=True), X_test_cat.reset_index(drop=True)], axis=1)\n",
    "    y_train = train_df['fare_per_minute']\n",
    "    y_test = test_df['fare_per_minute']\n",
    "\n",
    "    # Save expected columns\n",
    "    os.makedirs(\"ScoringModel/Models/expected_columns\", exist_ok=True)\n",
    "    expected_cols_path = f\"ScoringModel/Models/expected_columns/expected_columns_{model_type}.pkl\"\n",
    "    joblib.dump(X_train.columns.tolist(), expected_cols_path)\n",
    "\n",
    "    # Save raw X_train (optional)\n",
    "    if month_str:\n",
    "        os.makedirs(f\"Models/{month_str}\", exist_ok=True)\n",
    "        with open(f\"Models/{month_str}/X_train_{month_str}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(X_train, f)\n",
    "\n",
    "    # Train model\n",
    "    model = XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1) if model_type == 'xgb' else LGBMRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(f\"\\n {model_type.upper()} Model Results:\")\n",
    "    print(f\"R² Score: {r2_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"MAE: {mean_absolute_error(y_test, y_pred):.4f}\")\n",
    "\n",
    "    importance = model.feature_importances_\n",
    "    return model, dict(zip(X_train.columns, importance))\n",
    "\n",
    "# -------------------------------------\n",
    "# 3. SCORING FUNCTION\n",
    "# -------------------------------------\n",
    "def apply_scoring(df, model):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Define columns used in training\n",
    "    categorical_cols = ['is_airport_trip', 'pickup_borough', 'dropoff_borough']\n",
    "    numeric_cols = ['dropoff_zone_hotness', 'trip_duration_variability', 'sin_hour', 'cos_hour', 'is_weekend']\n",
    "\n",
    "    # One-hot encode categorical features\n",
    "    df_cat = pd.get_dummies(df[categorical_cols], drop_first=True)\n",
    "\n",
    "    # Align columns with model expectations\n",
    "    for col in model.feature_names_in_:\n",
    "        if col not in df_cat.columns and col not in df.columns:\n",
    "            df[col] = 0\n",
    "\n",
    "    X = pd.concat([df[numeric_cols], df_cat], axis=1)\n",
    "\n",
    "    # Reindex to match model input\n",
    "    X = X.reindex(columns=model.feature_names_in_, fill_value=0)\n",
    "\n",
    "    # Predict\n",
    "    df['predicted_score'] = model.predict(X)\n",
    "\n",
    "    # Normalize using 5th–95th percentile\n",
    "    p_min = df['predicted_score'].quantile(0.05)\n",
    "    p_max = df['predicted_score'].quantile(0.95)\n",
    "    df['final_score'] = ((df['predicted_score'] - p_min) / (p_max - p_min)).clip(0, 1)\n",
    "\n",
    "    # Save scaler range\n",
    "    scaler = {\"min\": float(p_min), \"max\": float(p_max)}\n",
    "\n",
    "    print(f\"\\n Score Normalization Range (5th–95th percentile): min = {p_min:.2f}, max = {p_max:.2f}\")\n",
    "    print(f\" Raw Predicted Score Stats:\\n  Min: {df['predicted_score'].min():.4f}, Max: {df['predicted_score'].max():.4f}\")\n",
    "    print(f\" Saved normalization range: min = {scaler['min']:.2f}, max = {scaler['max']:.2f}\")\n",
    "\n",
    "    return df, scaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------\n",
    "# 4. RUN PIPELINE\n",
    "# -------------------------------------\n",
    "def run_model_pipeline():\n",
    "    Tk().withdraw()\n",
    "    file_path = askopenfilename(title=\"Select Cleaned CSV with Features\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"Loaded:\", file_path)\n",
    "\n",
    "    df = prepare_model_data(df)\n",
    "   \n",
    "    df['pickup_date'] = pd.to_datetime(df['pickup_date'], errors='coerce')\n",
    "    df = df.dropna(subset=['pickup_date'])\n",
    "\n",
    "    # Get train/test months\n",
    "    month_order = df.groupby(df['pickup_date'].dt.month)['pickup_date'].min().sort_values().index.tolist()\n",
    "    month_a_num, month_b_num = month_order[0], month_order[1]\n",
    "    train_df = df[df['pickup_date'].dt.month == month_a_num]\n",
    "    test_df = df[df['pickup_date'].dt.month == month_b_num]\n",
    "    month_str = pd.to_datetime(f'2023-{month_b_num}-01').strftime('%B').lower()\n",
    "\n",
    "    print(\" Training months:\", month_a_num, \"→\", month_b_num)\n",
    "    print(\"Train size:\", train_df.shape[0], \"| Test size:\", test_df.shape[0])\n",
    "\n",
    "    # Train XGB\n",
    "    xgb_model, xgb_feats = train_model(train_df, test_df, model_type='xgb', month_str=month_str)\n",
    "    xgb_path = asksaveasfilename(initialfile=f\"model_{month_str}_xgb.pkl\", defaultextension=\".pkl\")\n",
    "    if xgb_path:\n",
    "        with open(xgb_path, \"wb\") as f:\n",
    "            pickle.dump(xgb_model, f)\n",
    "        print(\" XGB model saved to:\", xgb_path)\n",
    "\n",
    "    # Train LGB\n",
    "    lgb_model, lgb_feats = train_model(train_df, test_df, model_type='lgb', month_str=month_str)\n",
    "    lgb_path = asksaveasfilename(initialfile=f\"model_{month_str}_lgb.pkl\", defaultextension=\".pkl\")\n",
    "    if lgb_path:\n",
    "        with open(lgb_path, \"wb\") as f:\n",
    "            pickle.dump(lgb_model, f)\n",
    "        print(\" LGB model saved to:\", lgb_path)\n",
    "\n",
    "    # Feature importance\n",
    "    lgb_series = pd.Series(lgb_feats)\n",
    "    xgb_series = pd.Series(xgb_feats)\n",
    "    combined_df = pd.concat([xgb_series, lgb_series / lgb_series.sum()], axis=1, keys=['xgb', 'lgb_norm']).fillna(0)\n",
    "    combined_df['avg_importance'] = combined_df.mean(axis=1)\n",
    "    final_weights = combined_df['avg_importance'].sort_values(ascending=False)\n",
    "\n",
    "    # Save weights\n",
    "    weights_path = asksaveasfilename(initialfile=f\"scoring_weights_{month_str}.json\", defaultextension=\".json\")\n",
    "    if weights_path:\n",
    "        with open(weights_path, \"w\") as f:\n",
    "            json.dump(final_weights.to_dict(), f)\n",
    "        print(\" Weights saved to:\", weights_path)\n",
    "\n",
    "    # Score + scale\n",
    "    df_scored, scaler = apply_scoring(df, xgb_model)\n",
    "\n",
    "    # Save scaler\n",
    "    scaler_path = asksaveasfilename(initialfile=f\"scaler_{month_str}.json\", defaultextension=\".json\")\n",
    "\n",
    "    if scaler_path:\n",
    "        with open(scaler_path, \"w\") as f:\n",
    "            json.dump(scaler, f)\n",
    "        print(\" Scaler saved to:\", scaler_path)\n",
    "\n",
    "# Run it\n",
    "if __name__ == \"__main__\":\n",
    "    run_model_pipeline()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp47360Summerpy312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
