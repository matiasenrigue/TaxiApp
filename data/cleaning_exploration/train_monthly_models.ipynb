{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c0a5751",
   "metadata": {},
   "source": [
    "## Pipeline for Scoring Model\n",
    "\n",
    "In this notebook I made the final version of the pipeline needed to implement the scoring model and cleaned up the previous notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8ffb9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import pickle\n",
    "import json\n",
    "import joblib\n",
    "import os\n",
    "from tkinter import Tk\n",
    "from tkinter.filedialog import askopenfilename\n",
    "from tkinter.filedialog import asksaveasfilename\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30966902",
   "metadata": {},
   "source": [
    "## Monthly Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0564e52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select Month A CSV (used for training and variability calculation):\n",
      "Select Month B CSV (used to calculate hotness):\n",
      "\n",
      "Loaded Month A (Nov): 3072802 rows\n",
      "Loaded Month B (Dec): 3147996 rows\n",
      "\n",
      "Select where to save the hotness table\n",
      "Saved hotness table: C:/diksha/Summer Sem/ScoringModel/Models/December/hotness_table_dec.csv\n",
      "\n",
      "Select where to save the trip duration stats table\n",
      "Saved duration stats: C:/diksha/Summer Sem/ScoringModel/Models/December/duration_variability_nov_dec.csv\n",
      "\n",
      "Select location to save the final dataset\n",
      "Final file saved: C:/diksha/Summer Sem/ScoringModel/Data/Hotness and Duration/Data_with_Features_Nov_Dec.csv\n",
      "Final shape: (6220798, 35)\n"
     ]
    }
   ],
   "source": [
    "def run_feature_engineering_pipeline():\n",
    "   # File selection\n",
    "    Tk().withdraw()\n",
    "    print(\"Select Month A CSV (used for training and variability calculation):\")\n",
    "    file_a = askopenfilename()\n",
    "    print(\"Select Month B CSV (used to calculate hotness):\")\n",
    "    file_b = askopenfilename()\n",
    "\n",
    "    # Read files safely as strings\n",
    "    df_a = pd.read_csv(file_a, dtype=str)\n",
    "    df_b = pd.read_csv(file_b, dtype=str)\n",
    "\n",
    "    nyc_tz = pytz.timezone(\"America/New_York\")\n",
    "\n",
    "    for df in [df_a, df_b]:\n",
    "        df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'], errors='coerce')\n",
    "        df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'], errors='coerce')\n",
    "\n",
    "        # Localize naive timestamps to NYC time (assumes original times are NYC)\n",
    "        df['tpep_pickup_datetime'] = df['tpep_pickup_datetime'].dt.tz_localize(nyc_tz, ambiguous='NaT', nonexistent='shift_forward')\n",
    "        df['tpep_dropoff_datetime'] = df['tpep_dropoff_datetime'].dt.tz_localize(nyc_tz, ambiguous='NaT', nonexistent='shift_forward')\n",
    "\n",
    "\n",
    "    # # Confirm datetime parsing\n",
    "    # print(\"Month A pickup dtype:\", df_a['tpep_pickup_datetime'].dtype)\n",
    "    # print(\"Month B pickup dtype:\", df_b['tpep_pickup_datetime'].dtype)\n",
    "    # print(\"Month B NaT values:\", df_b['tpep_pickup_datetime'].isna().sum())\n",
    "\n",
    "    # Extract month names early\n",
    "    month_a = int(df_a['tpep_pickup_datetime'].dropna().dt.month.mode()[0])\n",
    "    month_b = int(df_b['tpep_pickup_datetime'].dropna().dt.month.mode()[0])\n",
    "    name_a = pd.to_datetime(f\"2023-{month_a}-01\").strftime('%b')\n",
    "    name_b = pd.to_datetime(f\"2023-{month_b}-01\").strftime('%b')\n",
    "\n",
    "    print(f\"\\nLoaded Month A ({name_a}): {df_a.shape[0]} rows\")\n",
    "    print(f\"Loaded Month B ({name_b}): {df_b.shape[0]} rows\")\n",
    "\n",
    "    # Combine both months\n",
    "    df_all = pd.concat([df_a, df_b], ignore_index=True)\n",
    "\n",
    "    # -------------------------------------\n",
    "    # Dropoff Zone Hotness Table\n",
    "    # -------------------------------------\n",
    "    df_all['pickup_day_of_week'] = df_all['tpep_pickup_datetime'].dt.dayofweek\n",
    "    df_all['pickup_hour'] = df_all['tpep_pickup_datetime'].dt.hour\n",
    "\n",
    "    df_hotness_source = df_all.dropna(subset=['pickup_day_of_week', 'pickup_hour', 'dropoff_zone'])\n",
    "\n",
    "    hotness_table = (\n",
    "        df_hotness_source\n",
    "        .groupby(['dropoff_zone', 'pickup_day_of_week', 'pickup_hour'])\n",
    "        .size()\n",
    "        .reset_index(name='dropoff_zone_hotness')\n",
    "    )\n",
    "\n",
    "    # Save hotness table\n",
    "    print(\"\\nSelect where to save the hotness table\")\n",
    "    hotness_path = asksaveasfilename(\n",
    "        initialfile=f\"hotness_table_{name_b.lower()}.csv\",\n",
    "        defaultextension=\".csv\",\n",
    "        filetypes=[(\"CSV files\", \"*.csv\")]\n",
    "    )\n",
    "    if hotness_path:\n",
    "        hotness_table.to_csv(hotness_path, index=False)\n",
    "        print(f\"Saved hotness table: {hotness_path}\")\n",
    "    else:\n",
    "        print(\"Hotness table save cancelled.\")\n",
    "\n",
    "    # Apply hotness to a dataset\n",
    "    def apply_hotness(df):\n",
    "        df['dropoff_day_of_week'] = df['tpep_dropoff_datetime'].dt.dayofweek\n",
    "        df['dropoff_hour'] = df['tpep_dropoff_datetime'].dt.hour\n",
    "        df = df.merge(\n",
    "            hotness_table,\n",
    "            left_on=['dropoff_zone', 'dropoff_day_of_week', 'dropoff_hour'],\n",
    "            right_on=['dropoff_zone', 'pickup_day_of_week', 'pickup_hour'],\n",
    "            how='left'\n",
    "        )\n",
    "        df['dropoff_zone_hotness'] = df['dropoff_zone_hotness'].fillna(0)\n",
    "        df['log_dropoff_zone_hotness'] = np.log1p(df['dropoff_zone_hotness'])\n",
    "        df.drop(columns=['pickup_day_of_week', 'pickup_hour'], errors='ignore', inplace=True)\n",
    "        return df\n",
    "\n",
    "    df_a = apply_hotness(df_a)\n",
    "    df_b = apply_hotness(df_b)\n",
    "\n",
    "    # -------------------------------------\n",
    "    # Trip Duration Variability Table\n",
    "    # -------------------------------------\n",
    "   # Ensure trip_duration_min is numeric\n",
    "    df_all['trip_duration_min'] = pd.to_numeric(df_all['trip_duration_min'], errors='coerce')\n",
    "\n",
    "    # Drop rows with missing duration before grouping\n",
    "    duration_group_cols = ['pickup_zone', 'dropoff_zone', 'pickup_day_of_week', 'pickup_hour']\n",
    "    duration_stats = (\n",
    "        df_all.dropna(subset=['trip_duration_min'])\n",
    "        .groupby(duration_group_cols)['trip_duration_min']\n",
    "        .agg(['mean', 'std'])\n",
    "        .reset_index()\n",
    "        .rename(columns={'std': 'trip_duration_variability'})\n",
    "    )\n",
    "\n",
    "    # Save duration variability stats\n",
    "    print(\"\\nSelect where to save the trip duration stats table\")\n",
    "    duration_path = asksaveasfilename(\n",
    "        initialfile=f\"duration_variability_{name_b.lower()}.csv\",\n",
    "        defaultextension=\".csv\",\n",
    "        filetypes=[(\"CSV files\", \"*.csv\")]\n",
    "    )\n",
    "    if duration_path:\n",
    "        duration_stats.to_csv(duration_path, index=False)\n",
    "        print(f\"Saved duration stats: {duration_path}\")\n",
    "    else:\n",
    "        print(\"Duration stats save cancelled.\")\n",
    "\n",
    "    def apply_variability(df):\n",
    "        df['pickup_day_of_week'] = df['tpep_pickup_datetime'].dt.dayofweek\n",
    "        df['pickup_hour'] = df['tpep_pickup_datetime'].dt.hour\n",
    "        df = df.merge(duration_stats, on=duration_group_cols, how='left')\n",
    "        df['trip_duration_variability'] = df['trip_duration_variability'].fillna(0)\n",
    "        return df\n",
    "\n",
    "    df_a = apply_variability(df_a)\n",
    "    df_b = apply_variability(df_b)\n",
    "\n",
    "    # -------------------------------------\n",
    "    # Final Save\n",
    "    # -------------------------------------\n",
    "    df_a['pickup_date'] = df_a['tpep_pickup_datetime'].dt.date\n",
    "    df_b['pickup_date'] = df_b['tpep_pickup_datetime'].dt.date\n",
    "    df_final = pd.concat([df_a, df_b], ignore_index=True)\n",
    "\n",
    "    print(\"\\nSelect location to save the final dataset\")\n",
    "    final_path = asksaveasfilename(\n",
    "        initialfile=f\"Data_with_Features_{name_a}_{name_b}.csv\",\n",
    "        defaultextension=\".csv\",\n",
    "        filetypes=[(\"CSV files\", \"*.csv\")]\n",
    "    )\n",
    "    if final_path:\n",
    "        df_final.to_csv(final_path, index=False)\n",
    "        print(f\"Final file saved: {final_path}\")\n",
    "        print(f\"Final shape: {df_final.shape}\")\n",
    "    else:\n",
    "        print(\"Final data save cancelled.\")\n",
    "\n",
    "# -------------------------------------\n",
    "# RUN IT\n",
    "# -------------------------------------\n",
    "run_feature_engineering_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44be696e",
   "metadata": {},
   "source": [
    "## Scoring and Training Code (XGBoost + LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f948979f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: C:/diksha/Summer Sem/ScoringModel/Data/Hotness and Duration/Data_with_Features_Jun_Jul.csv\n",
      " Training months: 6 → 7\n",
      "Train size: 3084442 | Test size: 2717185\n",
      "\n",
      " XGB Model Results:\n",
      "R² Score: 0.3389\n",
      "MAE: 0.1859\n",
      " XGB model saved to: C:/diksha/Summer Sem/ScoringModel/Models/July/model_july_xgb.pkl\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.074898 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 573\n",
      "[LightGBM] [Info] Number of data points in the train set: 3084442, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 1.210974\n",
      "\n",
      " LGB Model Results:\n",
      "R² Score: 0.3283\n",
      "MAE: 0.1877\n",
      " LGB model saved to: C:/diksha/Summer Sem/ScoringModel/Models/July/model_july_lgb.pkl\n",
      " Weights saved to: C:/diksha/Summer Sem/ScoringModel/Models/July/scoring_weights_july.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anar2\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Raw Predicted Score Stats:\n",
      "  Min: -0.1213\n",
      "  Max: 471.8494\n",
      " Filtered MinMaxScaler range: [0.00240646] to [99.9997561]\n",
      " Scaler saved to: C:/diksha/Summer Sem/ScoringModel/Models/July/scaler_july.pkl\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# 1. FEATURE PREP FUNCTION\n",
    "# -------------------------------------\n",
    "def prepare_model_data(df):\n",
    "    df = df.copy()\n",
    "    df['pickup_date'] = pd.to_datetime(df['pickup_date'])\n",
    "\n",
    "    # sin/cos hour\n",
    "    df['sin_hour'] = np.sin(2 * np.pi * df['pickup_hour'] / 24)\n",
    "    df['cos_hour'] = np.cos(2 * np.pi * df['pickup_hour'] / 24)\n",
    "    df.drop(columns=['pickup_hour', 'time_of_day'], inplace=True, errors='ignore')\n",
    "\n",
    "    # Ensure fare_per_minute is usable\n",
    "    df = df.dropna(subset=['fare_per_minute'])  # required target\n",
    "\n",
    "    return df\n",
    "\n",
    "# -------------------------------------\n",
    "# 2. TRAIN + EVAL FUNCTION\n",
    "# -------------------------------------\n",
    "def train_model(train_df, test_df, model_type='xgb', month_str=None):\n",
    "    categorical_cols = ['is_airport_trip', 'pickup_borough', 'dropoff_borough']\n",
    "    numeric_cols = [\n",
    "        'dropoff_zone_hotness', 'is_weekend',\n",
    "        'trip_duration_variability', 'sin_hour', 'cos_hour'\n",
    "    ]\n",
    "\n",
    "    # One-hot encode\n",
    "    X_train_cat = pd.get_dummies(train_df[categorical_cols], drop_first=True)\n",
    "    X_test_cat = pd.get_dummies(test_df[categorical_cols], drop_first=True)\n",
    "    X_test_cat = X_test_cat.reindex(columns=X_train_cat.columns, fill_value=0)\n",
    "\n",
    "    X_train = pd.concat([train_df[numeric_cols].reset_index(drop=True), X_train_cat.reset_index(drop=True)], axis=1)\n",
    "    X_test = pd.concat([test_df[numeric_cols].reset_index(drop=True), X_test_cat.reset_index(drop=True)], axis=1)\n",
    "    y_train = train_df['fare_per_minute']\n",
    "    y_test = test_df['fare_per_minute']\n",
    "\n",
    "    # Save expected columns\n",
    "    os.makedirs(\"ScoringModel/Models/expected_columns\", exist_ok=True)\n",
    "    expected_cols_path = f\"ScoringModel/Models/expected_columns/expected_columns_{model_type}.pkl\"\n",
    "    joblib.dump(X_train.columns.tolist(), expected_cols_path)\n",
    "\n",
    "    # Save raw X_train (optional)\n",
    "    if month_str:\n",
    "        os.makedirs(f\"Models/{month_str}\", exist_ok=True)\n",
    "        with open(f\"Models/{month_str}/X_train_{month_str}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(X_train, f)\n",
    "\n",
    "    # Train model\n",
    "    model = XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1) if model_type == 'xgb' else LGBMRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(f\"\\n {model_type.upper()} Model Results:\")\n",
    "    print(f\"R² Score: {r2_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"MAE: {mean_absolute_error(y_test, y_pred):.4f}\")\n",
    "\n",
    "    importance = model.feature_importances_\n",
    "    return model, dict(zip(X_train.columns, importance))\n",
    "\n",
    "# -------------------------------------\n",
    "# 3. SCORING FUNCTION\n",
    "# -------------------------------------\n",
    "def apply_scoring(df, final_weights):\n",
    "    df = df.copy()\n",
    "    for feat in final_weights:\n",
    "        if feat not in df.columns:\n",
    "            df[feat] = 0\n",
    "\n",
    "    df['predicted_score'] = df.apply(lambda row: sum(row.get(f, 0) * w for f, w in final_weights.items()), axis=1)\n",
    "\n",
    "    # Fit scaler on filtered predicted scores\n",
    "    scaler = MinMaxScaler()\n",
    "    filtered_scores = df['predicted_score'][(df['predicted_score'] > 0) & (df['predicted_score'] < 100)]\n",
    "    scaler.fit(filtered_scores.values.reshape(-1, 1))\n",
    "    df['final_score'] = scaler.transform(df[['predicted_score']])\n",
    "\n",
    "    print(f\"\\n Raw Predicted Score Stats:\\n  Min: {df['predicted_score'].min():.4f}\\n  Max: {df['predicted_score'].max():.4f}\")\n",
    "    print(f\" Filtered MinMaxScaler range: {scaler.data_min_} to {scaler.data_max_}\")\n",
    "\n",
    "    return df, scaler\n",
    "\n",
    "# -------------------------------------\n",
    "# 4. RUN PIPELINE\n",
    "# -------------------------------------\n",
    "def run_model_pipeline():\n",
    "    Tk().withdraw()\n",
    "    file_path = askopenfilename(title=\"Select Cleaned CSV with Features\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"Loaded:\", file_path)\n",
    "\n",
    "    df = prepare_model_data(df)\n",
    "   \n",
    "    df['pickup_date'] = pd.to_datetime(df['pickup_date'], errors='coerce')\n",
    "    df = df.dropna(subset=['pickup_date'])\n",
    "\n",
    "    # Get train/test months\n",
    "    month_order = df.groupby(df['pickup_date'].dt.month)['pickup_date'].min().sort_values().index.tolist()\n",
    "    month_a_num, month_b_num = month_order[0], month_order[1]\n",
    "    train_df = df[df['pickup_date'].dt.month == month_a_num]\n",
    "    test_df = df[df['pickup_date'].dt.month == month_b_num]\n",
    "    month_str = pd.to_datetime(f'2023-{month_b_num}-01').strftime('%B').lower()\n",
    "\n",
    "    print(\" Training months:\", month_a_num, \"→\", month_b_num)\n",
    "    print(\"Train size:\", train_df.shape[0], \"| Test size:\", test_df.shape[0])\n",
    "\n",
    "    # Train XGB\n",
    "    xgb_model, xgb_feats = train_model(train_df, test_df, model_type='xgb', month_str=month_str)\n",
    "    xgb_path = asksaveasfilename(initialfile=f\"model_{month_str}_xgb.pkl\", defaultextension=\".pkl\")\n",
    "    if xgb_path:\n",
    "        with open(xgb_path, \"wb\") as f:\n",
    "            pickle.dump(xgb_model, f)\n",
    "        print(\" XGB model saved to:\", xgb_path)\n",
    "\n",
    "    # Train LGB\n",
    "    lgb_model, lgb_feats = train_model(train_df, test_df, model_type='lgb', month_str=month_str)\n",
    "    lgb_path = asksaveasfilename(initialfile=f\"model_{month_str}_lgb.pkl\", defaultextension=\".pkl\")\n",
    "    if lgb_path:\n",
    "        with open(lgb_path, \"wb\") as f:\n",
    "            pickle.dump(lgb_model, f)\n",
    "        print(\" LGB model saved to:\", lgb_path)\n",
    "\n",
    "    # Feature importance\n",
    "    lgb_series = pd.Series(lgb_feats)\n",
    "    xgb_series = pd.Series(xgb_feats)\n",
    "    combined_df = pd.concat([xgb_series, lgb_series / lgb_series.sum()], axis=1, keys=['xgb', 'lgb_norm']).fillna(0)\n",
    "    combined_df['avg_importance'] = combined_df.mean(axis=1)\n",
    "    final_weights = combined_df['avg_importance'].sort_values(ascending=False)\n",
    "\n",
    "    # Save weights\n",
    "    weights_path = asksaveasfilename(initialfile=f\"scoring_weights_{month_str}.json\", defaultextension=\".json\")\n",
    "    if weights_path:\n",
    "        with open(weights_path, \"w\") as f:\n",
    "            json.dump(final_weights.to_dict(), f)\n",
    "        print(\" Weights saved to:\", weights_path)\n",
    "\n",
    "    # Score + scale\n",
    "    df_scored, scaler = apply_scoring(df, final_weights)\n",
    "\n",
    "    # Save scaler\n",
    "    scaler_path = asksaveasfilename(initialfile=f\"scaler_{month_str}.pkl\", defaultextension=\".pkl\")\n",
    "    if scaler_path:\n",
    "        with open(scaler_path, \"wb\") as f:\n",
    "            pickle.dump(scaler, f)\n",
    "        print(\" Scaler saved to:\", scaler_path)\n",
    "\n",
    "# Run it\n",
    "if __name__ == \"__main__\":\n",
    "    run_model_pipeline()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp47360Summerpy312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
